{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of anomaly detection is to identify example sets which look like outliers with respect to the bulk of data. The key assumption is that there are at least two non-overlapping underlying statistical distributions with the examples we are interested in, the anomalies, coming from a rarely sampled distribution with respect to the bulk of the data.\n",
    "\n",
    "In a real world analysis we would usually not know which data are from which distribution, at least until we start investigating the cause of statistical anomalies in the field through some kind of inspection process. In this workshop you have been provided with the class labels which effectively label the different distributions. That means in principle you can implement a supervised learning approach to get better performance, or at least bias the training data in favour of the distribution you would like to learn in an unsupervised or semi-supervised learning approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of multivariate time series. Each time series is from a different turbofan engine. The data can be considered to be taken from a fleet of engines of the same type. Each engine starts with different degrees of initial wear and manufacturing variation which is unknown to the user. This wear and variation is considered normal, i.e., it is not considered a fault condition. There are three operational settings that have a substantial effect on engine performance. These settings are also included in the data. The sensor data are contaminated with noise. \n",
    "\n",
    "The data set is divided into training, validation, and testing subsets. Whereas the training and validation sets will be provided and used for data exploration and model development, we will use the test set for evaluation of the models that you will have built. \n",
    "\n",
    "The ground truth data provides the number of remaining working cycles for the engines. The engine is operating normally at the start of each time series, and develops a fault at some point in time during the series. In the training set, the fault grows in magnitude until system failure. In the validation and test sets, the time series ends some time prior to system failure. \n",
    "\n",
    "The data are provided in csv-formatted text file with 28 columns. Each row is a snapshot of data taken during a single operational cycle, each column is a different variable. The columns correspond to:\n",
    "\n",
    "- id: ID of the time series (one time series ~ one equipment)\n",
    "- unit number\n",
    "- time, in cycles\n",
    "- operational setting 1\n",
    "- operational setting 2\n",
    "- operational setting 3\n",
    "- sensor measurement 1\n",
    "- sensor measurement 2\n",
    "- ...\n",
    "- sensor measurement 21\n",
    "- RUL, remaining useful lifetime \n",
    "- label, 0 for normal and 1 for anomalies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/data_overview.png\" width=50% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset itself does not have any directly applied class label. However, we know that an equipment develops a fault at some point which lasts until the end of the series, corresponding to a system failure. Based on that, We have assigned a label of anomaly (coded as 1) is assigned to samples that fall within a time window of 30 cycles prior to the system failure. All other samples are considered normal, coded as 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of the workshop is to go through an end-to-end workflow of data exploration, building a model that detects anomalies with the highest possible true positive rate whereas keeping false alarm rate to a minimum, deploying the trained model in a cloud-based production environment, and evaluating its performance when running in production with unseen data. \n",
    "\n",
    "\n",
    "Failure in heavy-asset equipments is rare and catastrophic. In practice, when faced with the challenges of detecting failures for heavy-asset equipments, we likely do not have access to labeled failures or have access to such a limited  number of labels that it is impossible to use supervised approaches. \n",
    "\n",
    "Thus, for the competition in this workshop, despite that class label is provided in the training and validation sets, and that we will explore both supervised and unsupervised approaches, we will **only consider unsupervised or semi-supervised methods**, where the labels are not directly used while training a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance will be graded using the [f1-score](https://en.wikipedia.org/wiki/F1_score). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load data: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(data_dir.joinpath('train.csv'))\n",
    "validation_df = pd.read_csv(data_dir.joinpath('validation.csv'))\n",
    "print(train_df.shape, validation_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['setting{}'.format(i) for i in range(1, 4)]\n",
    "features += ['s{}'.format(i) for i in range(1, 22)]\n",
    "\n",
    "X_train, y_train = train_df[features], train_df['label']\n",
    "X_validation, y_validation = validation_df[features],  validation_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion between anomaly and normal training samples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ratio = y_train.value_counts()[1] / len(y_train)\n",
    "print(class_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training:\", train_df.isnull().values.any())\n",
    "print(\"Test:\", validation_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at some basic statistics of each of the column in each set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('id').describe().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.groupby('id').describe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations and remarks: \n",
    "\n",
    "- `s18` and `s19` are constant, both in the training and in the validation sets, suggesting that the operating condition did not change and/or these sensors was broken/inactive. We may disregard these variables from the analyses.\n",
    "- Columns `s1`, `s5`, `s10` and `s16` are mostly constant and may be excluded from analyses too. \n",
    "- The statistics on the number of cycles are not relevant, as we should only look at the statistics for the maximum number of cycles for each engine. However, the mean and median number of cycles for the training set are larger than for the validation set, confirms the fact that in the training set, the engines run until system failure, wheras in the validation set, the time series ends prior to system failure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the sensors as a function of remaining useful life (RUL):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "g = sns.PairGrid(data=train_df.query('id < 10') ,\n",
    "                 x_vars=['RUL'],\n",
    "                 y_vars=['s1', 's2', 's3', 's4'],\n",
    "                 hue=\"id\", size=3, aspect=2.5)\n",
    "g = g.map(plt.scatter, alpha=0.5)\n",
    "g = g.set(xlim=(300,0))\n",
    "g = g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: explore the relation of other sensors and RUL as well as the inter-correlation among the sensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build anomaly detection models using unsupervised approaches, i.e. the class labels are not used during this process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the features so that they are in the same range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# fit the scaler using the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# apply the scaler to both the training and validation sets\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_validation_norm = scaler.transform(X_validation)\n",
    "\n",
    "print(X_train_norm.shape, X_validation_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneclass_SVM = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma='auto')\n",
    "robust_covariance = EllipticEnvelope(contamination=0.05)\n",
    "\n",
    "models_unsupervised = {\n",
    "    'OneClassSVM': oneclass_SVM,\n",
    "    'RobustCovariance': robust_covariance\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit the models using the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models_unsupervised.items():\n",
    "    print(\"Fitting \", model_name)\n",
    "    models_unsupervised[model_name] = model.fit(X_train_norm)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: build an anomaly detection model based on a different algorithm such as IsolationForest or Local Outlier Factor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our trained models to the validation set and calculate the model performance using accuracy and area under the ROC curve (AUC): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def performance(yhat, y, score=None): \n",
    "    \"\"\"calculate various model evaluation metrics\n",
    "    Args: \n",
    "        yhat (binary): predicted label\n",
    "        y (binay): true label\n",
    "        score (float): anomaly score\n",
    "    Returns: \n",
    "        dictionary containing different evaluation metrics\n",
    "    \"\"\"\n",
    "    performance = {}\n",
    "    performance['accuracy'] = metrics.accuracy_score(y, yhat)\n",
    "    if score is not None: \n",
    "        FP, TP, _ = metrics.roc_curve(y, score)\n",
    "        performance['FP_TP'] = (FP, TP)\n",
    "        performance['AUC'] = metrics.auc(FP, TP)\n",
    "        performance['yhat'] = yhat\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for plotting confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confmat(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix and save to user_files dir\n",
    "    \"\"\"\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(7,7))\n",
    "    sns.heatmap(conf_matrix,\n",
    "                annot=True,\n",
    "                fmt='.0f')\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained model to make detection on the validations set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_unsupervised = {}\n",
    "for model_name, model in models_unsupervised.items():\n",
    "    # predict\n",
    "    pred = model.predict(X_validation_norm)\n",
    "    yhat = np.where(pred == -1, 1, 0)\n",
    "    anomaly_score = - model.decision_function(X_validation_norm)\n",
    "    perf = performance(yhat, y_validation, score=anomaly_score)\n",
    "    results_unsupervised[model_name] = perf\n",
    "\n",
    "    print('{model}: accuracy = {acc}, AUC = {auc}'.format(model=model_name, acc=perf['accuracy'], auc=perf['AUC']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain more insights on the performance of the model on each class, let us take a look at the confusion matrix: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confmat(y_validation, results_unsupervised['OneClassSVM']['yhat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy can be misleading particularly in the case of highy imbalanced data. In which cases, other metrics such as precision, recall and f1_score should be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: add precision, recall, [precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html), and f1_score metrics to the `performance` function (ref. [commonly used evaluation metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import LocalOutlierFactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the default values of the hyperparameters. Potentially, an improved model can be built by tuning the hyperparameters of an algorithm using cross-validation on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "knc = KNeighborsClassifier()\n",
    "svc = SVC(gamma='auto', probability=True)\n",
    "\n",
    "models_supervised = {\n",
    "    'LogisticRegression': lr,\n",
    "    'RandomForestClassifier':rfc,\n",
    "    'KNeighborsClassifier': knc,\n",
    "    'SVC': svc\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fit the models using the training data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models_supervised.items():\n",
    "    print(\"Fitting \", model_name)\n",
    "    models_supervised[model_name] = model.fit(X_train_norm, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the trained model on the validation data and calculate the performance. Note that we will make use of the `performance` function introduced in the \"Unsupervised learning\" section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_supervised = {}\n",
    "for model_name, model in models_supervised.items():\n",
    "    # predict\n",
    "    yhat = model.predict(X_validation_norm)\n",
    "    anomaly_score = model.predict_proba(X_validation_norm)[:, 1]\n",
    "    perf = performance(yhat, y_validation, score=anomaly_score)\n",
    "    results_supervised[model_name] = perf\n",
    "\n",
    "    print('{model}: accuracy = {acc}, AUC = {auc}'.format(model=model_name, acc=perf['accuracy'], auc=perf['AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_confmat(y_validation, results_supervised['KNeighborsClassifier']['yhat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning-based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the algorithms presented above, which are rather conventional machine learning algorithms, one may explore the use of deep learning-based algorithms as potential performant alternatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will experiment with the **autoencoder** as an unsupervised deep learning-based approach for anomaly detection. \n",
    "\n",
    "The autoencoder model aims to derive a data driven representation of the identity function. Given that by far the majority of the training data are of class normal we might expect this identity mapping to perform well for that class and less well for the class where we have far fewer training examples. It follows that if we calculate the performance of the mapping using, e.g., mean squared error, and determine a large quantile (e.g. 95%) of the performance we might start to identify the anomalies. \n",
    "\n",
    "\n",
    "First we construct the model. We will use a shallow autoencoder with no hyperparameter optimization (the setup of the hidden layer is fairly arbitrary and there may be significant room for improvement). As the model will have number of parameters comparable to a significant fraction of the training dataset we feel it's necessary to implement regularization to avoid overfitting; we use simple L1 but it might be interesting to see if dropout can improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "# input layer\n",
    "input_layer = Input(shape=(len(features), ))\n",
    "\n",
    "# encoding layer with arbitrary hyperparameters and L1 regularization\n",
    "encoder = Dense(\n",
    "    20, activation=\"relu\",\n",
    "    activity_regularizer=regularizers.l1(10e-5)\n",
    ")(input_layer)\n",
    "\n",
    "# decoding layer\n",
    "decoder = Dense(len(features), activation='relu')(encoder)\n",
    "\n",
    "# book the model and summarize \n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next configure and train the model, we will save the model for later use, note this introduces a hidden dependency to h5py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "# require model be saved and logged\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\", verbose=0, save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "# compile the model for training\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "# train\n",
    "history = (\n",
    "    autoencoder.fit(\n",
    "        X_train_norm, X_train_norm,\n",
    "        epochs=n_epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        verbose=0, #set to 1 to monitor the fitting progress\n",
    "        validation_data=[X_validation_norm, X_validation_norm],\n",
    "        callbacks=[checkpointer, tensorboard]\n",
    "    ).history\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model, let's plot the loss evolution as a function of training input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['val_loss'])\n",
    "plt.plot(history['loss'])\n",
    "plt.legend(['val_loss','loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a considerable gap between the validation and training datasets, which indicates a potential for improving the model. Note however that the loss on the validation set is lower than that on the training set, which may appear to be counter-intuitive. \n",
    "\n",
    "**Exercise**: compare the class proportion (anomaly vs. normal) between the training and validation sets to find an explanation from a data-characteristics point of view to the difference in loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use the train data to determine the 95% threshold which assigns an anomaly and see how that impact the performance of anomaly detection on the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "trained_model = load_model('model.h5')\n",
    "train_predicted = trained_model.predict(X_train_norm)\n",
    "train_mse = mean_squared_error(train_predicted.T, X_train_norm.T, multioutput='raw_values')\n",
    "thresh_train = np.percentile(train_mse, 95)\n",
    "print(thresh_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "# propagate the scaled validation data through prediction workflow\n",
    "val_predicted = trained_model.predict(X_validation_norm)\n",
    "val_mse = mean_squared_error(val_predicted.T, X_validation_norm.T, multioutput='raw_values')\n",
    "yhat = [0 if imse < thresh_train else 1 for imse in val_mse]\n",
    "performance(yhat, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confmat(yhat, y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: to avoid overfitting while tuning the model, split the original training set into two subsets: one used for training and the other one used for validation. The original validation would be used an an independent test set to evaluate the final model. Repeat the above analyses. Experiment with different quantile thresholds and network architectures and observe how the changes affect the performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a model is successfuly built with satisfying performance as quantified by an evaluation metric, it should be persisted for use in production to perform anomaly detection on unseen production data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model can be persisted/saved as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "model_name = 'RandomForestClassifier'\n",
    "model = models_supervised[model_name]\n",
    "\n",
    "joblib.dump(model, '{}.model'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that any preprocessing steps performed prior to model training, such as feature scaling, should also be persisted to be used on the production data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(scaler, 'scaler.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will compare the performance of all models that have been built so far: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {**results_supervised, **results_unsupervised}\n",
    "\n",
    "metrics_ = ['accuracy', 'AUC']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for model, result in results.items():\n",
    "    df = df.append(\n",
    "        pd.DataFrame(\n",
    "            data={ key: result[key] for key in metrics_ },\n",
    "            index=[model]\n",
    "        )\n",
    "    )\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC curve: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['OneClassSVM', 'LogisticRegression']\n",
    "results_ = [results[model_name] for model_name in model_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils.fixes import signature\n",
    "\n",
    "step_kwargs = (\n",
    "    {'step': 'post'}\n",
    "    if 'step' in signature(plt.fill_between).parameters\n",
    "    else {}\n",
    ")\n",
    "\n",
    "_, ax = plt.subplots(1, len(model_names), figsize=(10,5))\n",
    "\n",
    "for n, result in enumerate(results_):\n",
    "    FP, TP = result['FP_TP']\n",
    "    ax[n].step(FP, TP, color='b', alpha=0.2, where='post')\n",
    "    ax[n].fill_between(FP, TP, alpha=0.2, color='b', **step_kwargs)\n",
    "    ax[n].set_xlabel('FP rate')\n",
    "    ax[n].set_ylabel('TP rate')\n",
    "    ax[n].set_ylim([0.0, 1.05])\n",
    "    ax[n].set_xlim([0.0, 1.0])\n",
    "    ax[n].set_title('{} - ROC curve'.format(model_names[n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot clearly shows a significant difference in performance between supervised and unsupervised learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: plot the precision-recall curve in addition to the ROC curve for all models for comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your best model given the training and validation data and output your model, as well as any preprocessing steps needed, for the deployment step. \n",
    "\n",
    "Potential avenues for performance improvement: \n",
    "- Explore the use of other anomaly detection approaches (ref. [pyod](https://pyod.readthedocs.io/en/latest/), [scikit-learn](https://scikit-learn.org/stable/modules/outlier_detection.html)).\n",
    "- Feature reduction using techniques such as Principal Component Analysis (PCA) or feature selection. \n",
    "- Resolve data imbalanced issue using data imputation.\n",
    "- Model hyperparameter tuning using cross-validation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB: only unsupervised or semi-supervised approaches will be eligible for the competition**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the analyses presented in this notebook, we have learned about: \n",
    "- End-to-end iterative model development workflow applied to anomaly detection using data of a physical equipment, consisting of: \n",
    "    - Exploratory data analysis,\n",
    "    - Model training,\n",
    "    - Model persistence for deployment. \n",
    "- A wide variety of anomaly detection approaches (supervised, unsupervised). \n",
    "- Different metrics that can be used for quantifying the performance of an anomaly detection model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
